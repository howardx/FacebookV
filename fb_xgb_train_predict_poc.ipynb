{
 "metadata": {
  "name": "",
  "signature": "sha256:88b8aedd34e802c1dfed8bbdd106b27791bc29741a3b58cd5e19a4348ca7227c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy\n",
      "\n",
      "import xgboost as xgb\n",
      "\n",
      "from sklearn import cross_validation, metrics # Additional scklearn functions\n",
      "from sklearn.grid_search import GridSearchCV # Perforing grid search"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df = pd.read_csv('data/train_100K_sample.csv')\n",
      "\n",
      "inputOpts = dict(delimiter = \",\",\n",
      "               dtype = float,\n",
      "               skip_header = 1)\n",
      "\n",
      "# it's NOT possible to have a standard 2d array while also having named columns in numpy - with names it has bo be structured array\n",
      "bigdata = np.genfromtxt('data/train_10K_sample.csv', **inputOpts)\n",
      "\n",
      "data = bigdata[0:100, :] # xgboost can be slow\n",
      "\n",
      "print data.shape\n",
      "sz = data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(100, 6)\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# xgboos expect labels/classes to be in the format of a series of integers starting from 0 (0, 1, 2, 3, ...)\n",
      "- Hence preprocess is needed to convert classes/labels to factors, with mapping info for generate final results.\n",
      "- a quick test for pandas.factorize() function, which should solve this problem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.array([-0.2, 23, 23, 23, 23, 2, 3, 4, -1, 2.3, 3.4])\n",
      "\n",
      "labels, classes = pd.factorize(a)\n",
      "\n",
      "print labels # list of classes/labels converted to factors that start from 0 (0, 1, 2...) - what xgboost needs\n",
      "print classes # list of unique classes/labels/factors, ordering/index is based on factor value assigned in \"labels\" list (first return)\n",
      "\n",
      "print len(labels) == len(a)\n",
      "print len(classes) == len(np.unique(a))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 1 1 1 2 3 4 5 6 7]\n",
        "[ -0.2  23.    2.    3.    4.   -1.    2.3   3.4]\n",
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Steps to convert class/label column into xgboost-compatible format\n",
      "- take the class/label column out (remove) from original matrix, save it somewhere for verification\n",
      "- use Pandas.factorize() to perform factorization to the class/label column, this function return 2 numpy.ndarray\n",
      "    - first returned numpy.ndarray is a list of factorized class/label (xgboost compatible)\n",
      "    - second returned numpy.ndarray is a list of unique class/label, ordered/indexed according to factorized values of original class/label (can be used as a reference for look-up)\n",
      "- append the factorized class/label back to the original matrix (here if we are using numpy.ndarray we may need to perform numpy.reshape() before append column-wise\n",
      "\n",
      "# After forecasting xgboost will return class/label in factorized format, need to convert back\n",
      "- use elements in the xgboost returned list as index to access/lookup the second returned numpy.ndarray from Pandas.factorize()\n",
      "- results gathered will be in the original class/label, and can be used as final results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "original_class = data[:,5] # original class labels before factorization\n",
      "data = scipy.delete(data, 5, 1)  # delete the original class label column - cannot be used by xgboost\n",
      "\n",
      "factorized_class, unique_class = pd.factorize(original_class)\n",
      "overall_num_of_classes = len(unique_class)\n",
      "print overall_num_of_classes\n",
      "\n",
      "# reshape so dimension matches for appending to original dataset\n",
      "fclass_np_horizontal = np.reshape(factorized_class, (sz[0], 1)) \n",
      "print fclass_np_horizontal.shape\n",
      "\n",
      "# add factorized labels as the last column of original matrix\n",
      "data = np.hstack((data, fclass_np_horizontal)) \n",
      "\n",
      "# sanity check for 2 way conversion of factorization of classes and vise versa\n",
      "original_class_back_from_factorized = [unique_class[i] for i in data[:, 5]]\n",
      "\n",
      "unequal = 0\n",
      "for i in range(len(original_class_back_from_factorized)):\n",
      "    if original_class[i] != original_class_back_from_factorized[i]:\n",
      "        unequal = unequal + 1\n",
      "print unequal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100\n",
        "(100, 1)\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# train/validation/test split - 70%:20%:10%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_train = sz[0] * 0.7\n",
      "num_validation = sz[0] * 0.2\n",
      "# num_test will be all the remaining\n",
      "\n",
      "train_validation_test = np.split(data, [num_train, num_train + num_validation], axis = 0)\n",
      "\n",
      "train = train_validation_test[0]\n",
      "validation = train_validation_test[1]\n",
      "test = train_validation_test[2]\n",
      "\n",
      "print train.shape\n",
      "print validation.shape\n",
      "print test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(70, 6)\n",
        "(20, 6)\n",
        "(10, 6)\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_X = train[:,0:4]\n",
      "train_Y = train[:, 5]\n",
      "print train_X.shape\n",
      "\n",
      "validation_X = validation[:,0:4]\n",
      "validation_Y = validation[:, 5]\n",
      "print validation_Y.shape\n",
      "\n",
      "test_X = test[:,0:4] # for testing set normally we don't have Y/predictor information, hence skipping test_Y\n",
      "print test_X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(70, 4)\n",
        "(20,)\n",
        "(10, 4)\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert data to xgboost specific structure - DMatrix\n",
      "xg_train = xgb.DMatrix( train_X, label = train_Y)\n",
      "xg_validation = xgb.DMatrix(validation_X, label = validation_Y)\n",
      "xg_test = xgb.DMatrix(test_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup parameters for xgboost with a python dictionary\n",
      "param = {}\n",
      "\n",
      "# use softmax multi-class classification\n",
      "param['objective'] = 'multi:softprob' # tells boosted trees to output probability\n",
      "\n",
      "# param[booster] - default is set to \"gbtree\" - gradient boosted tree\n",
      "\n",
      "# scale weight of positive examples\n",
      "param['eta'] = 0.1 # Parameters for Tree Booster - Booster parameter\n",
      "\n",
      "param['max_depth'] = 3 # Parameters for Tree Booster - Booster parameter\n",
      "\n",
      "param['silent'] = 1 # whether to print logs\n",
      "\n",
      "param['nthread'] = 4 # parallelism\n",
      "\n",
      "param['eval_metric'] = 'mlogloss'\n",
      "\n",
      "param['num_class'] = overall_num_of_classes # number of classes "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# specify traing/testing set for model training \n",
      "watchlist = [ (xg_train,'train'), (xg_validation, 'validation') ]\n",
      "\n",
      "# specify the number of weak classifiers (base boosters) in the ensemble #\n",
      "num_round = 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train the model\n",
      "bst_return_prob = xgb.train(param, xg_train, num_round, watchlist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[0]\ttrain-merror:0.985714\tvalidation-merror:1.000000\n",
        "[1]\ttrain-merror:0.985714\tvalidation-merror:1.000000\n",
        "[2]\ttrain-merror:0.985714\tvalidation-merror:1.000000\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prediction for testing set\n",
      "preds_prob = bst_return_prob.predict(xg_test)\n",
      "print preds_prob.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10, 100)\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# with \"softmax\" objective we'll get one class back - the class with maximum probability from tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup parameters for xgboost with a python dictionary\n",
      "param2 = {}\n",
      "# use softmax multi-class classification\n",
      "param2['objective'] = 'multi:softmax'\n",
      "\n",
      "# scale weight of positive examples\n",
      "param2['eta'] = 0.1 # Parameters for Tree Booster - Booster parameter\n",
      "param2['max_depth'] = 3 # Parameters for Tree Booster - Booster parameter\n",
      "param2['eval_metric'] = 'merror'\n",
      "param2['num_class'] = overall_num_of_classes # number of classes "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# specify traing/testing set for model training - testing set here used as validation set\n",
      "watchlist = [ (xg_train,'train'), (xg_validation, 'validation') ]\n",
      "num_round = 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train the model\n",
      "bst_return_max = xgb.train(param2, xg_train, num_round, watchlist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[0]\ttrain-merror:0.985714\tvalidation-merror:1.000000\n",
        "[1]\ttrain-merror:0.985714\tvalidation-merror:1.000000\n",
        "[2]\ttrain-merror:0.985714\tvalidation-merror:1.000000\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is prediction\n",
      "preds_max = bst_return_max.predict(xg_test)\n",
      "print preds_max.shape\n",
      "print preds_max"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10,)\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# use output from \"softprob\" objective, and generate final outputs in original class/label from factorized class/label"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sortedProbIdx = np.argsort(preds_prob)\n",
      "\n",
      "top3prob = sortedProbIdx[:, -3:]\n",
      "\n",
      "print top3prob\n",
      "print top3prob.shape\n",
      "\n",
      "# lot of values are duplicates, so index returned are arbitrary for duplicate values\n",
      "print preds_prob[0, [99, 98, 97, 80, 81]]\n",
      "print preds_prob[0, [26, 24, 49, 0, 1, 2]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]\n",
        " [26 24 49]]\n",
        "(10, 3)\n",
        "[ 0.00915875  0.00915875  0.00915875  0.00915875  0.00915875]\n",
        "[ 0.01036053  0.01036053  0.01036053  0.01036053  0.01036053  0.01036053]\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Steps to convert xgboost results to final result ( factorized label to original label )\n",
      "   - make sure the stuff needed are at hand, they are:\n",
      "       - two return values from pandas.factorize()\n",
      "           - a list of factorized class/label, equal length as training set, 1 to 1 mapping ( input for xgboost )\n",
      "           - a list of unique original class/label, indexed based on factorized class/label\n",
      "       - result from xgboost ( objective = softprob ) in the format of a N x M matrix:\n",
      "           - N, the number of rows, equal to the length of testing set\n",
      "           - M, the number of columns, equal to the number of unique class/label in training set, each value is a probability for that class/label being the chosen one\n",
      "           - because xgboost always require class/label to be in the format of 0, 1, 2, 3... so for example row 3 column 9 has value of 0.456, this means that for testing set data point 3, the probability of this data point to be classified as class/label 9 is 0.456\n",
      "   - apply numpy.argsort() to the result from xgboost, result will be also a N x M matrix:\n",
      "       - N, the number of rows, equal to the length of testing set\n",
      "       - M, the number of columns, equal to the number of unique class/label in training set, each value is a list index; if we fill a new list with these index, using values from corresponding row in xgboost output, it'll be a sorted list in ascending order \n",
      "   - use the output from numpy.argsort(), take the last k columns \n",
      "       - Result from numpy.argsort() is the indices that would sort an array in ascending order, so the last k columns represents the largest values' indices\n",
      "       - topK = argsortResult[:, -k:]\n",
      "       - topK will be a N x K matrix:\n",
      "           - N, the number of rows, equal to the length of testing set\n",
      "           - K, the number of columns, equal to the number of largest values we want in final results. \n",
      "   - use each row in topK matrix, as indices to access the second output from pandas.factorize(), a list of unique original class/label, which is indexed based on factorized class/label ==> result will be in original label, what you'll need"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# getting back to the 2nd output from pandas.factorize()\n",
      "print unique_class.shape # number of unique classes, ordered/indexed according to factorization label\n",
      "\n",
      "print unique_class[[26, 24, 49]]\n",
      "\n",
      "finalResults = unique_class[top3prob]\n",
      "\n",
      "print finalResults.shape\n",
      "print finalResults"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  6.47379404e+09   7.94795592e+09   6.06079623e+09   5.91448414e+09\n",
        "   8.99927996e+09]\n",
        "(100,)\n",
        "[  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        "(10, 3)\n",
        "[[  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]\n",
        " [  1.91208564e+09   8.47654971e+09   7.43347756e+09]]\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}