{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import tee, izip\n",
    "import os\n",
    "\n",
    "import ConfigParser\n",
    "\n",
    "from multiprocessing import Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment variables store in config file - depends on where we run this ntbk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/Kaggle/facebook_Jul_2016/FacebookV/params/param_1.conf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser.ConfigParser()\n",
    "#config.read(\"/user_home/w_howardx/git/FacebookV/config/wakari.conf\") # wakari\n",
    "config.read(\"/home/ec2-user/Kaggle/facebook_Jul_2016/FacebookV/config/aws.conf\") # AWS\n",
    "\n",
    "param_obj = ConfigParser.ConfigParser()\n",
    "param_obj.read(config.get(\"path\", \"param_file\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/Kaggle/facebook_Jul_2016/feature_engineered_input/hr_wk_mth/50_50_grid/test/\n",
      "['path']\n",
      "['train_file', 'test_file', 'param_file', 'train_grid_path', 'test_grid_path', 'model_bite_path', 'model_txt_path', 'grid_output_path', 'output_path']\n"
     ]
    }
   ],
   "source": [
    "print config.get(\"path\", \"test_grid_path\")\n",
    "print config.sections()\n",
    "print config.options(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'silent': '1', 'eval_metric': 'mlogloss', 'nthread': '4', 'eta': '0.1', 'objective': 'multi:softprob', 'booster': 'gbtree'}\n"
     ]
    }
   ],
   "source": [
    "param = dict(param_obj.items('param'))\n",
    "print param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29118021, 9)\n",
      "   row_id       x       y  accuracy    time    place_id  hour  weekday  month\n",
      "0       0  0.7941  9.0809        54  470702  8523065625    22        5     11\n",
      "1       1  5.9567  4.7968        13  186555  1757726713    14        4      5\n",
      "2       2  8.3078  7.0407        74  322648  1137537235     2        1      8\n",
      "3       3  7.3665  2.5165        65  704587  6567393236     8        7      5\n",
      "4       4  4.0961  1.1307        31  472130  7440663949    21        6     11\n",
      "   row_id       x       y  accuracy    time  hour  weekday  month\n",
      "0       0  0.1675  1.3608       107  930883    11        3     10\n",
      "1       1  7.3909  2.5301        35  893017     4        5      9\n",
      "2       2  8.0978  2.3473        62  976933    11        7     11\n",
      "3       3  0.9990  1.0591        62  907285     2        1     10\n",
      "4       4  0.6670  9.7254        40  914399    24        5     10\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(config.get(\"path\", \"train_file\"))\n",
    "test = pd.read_csv(config.get(\"path\", \"test_file\"))\n",
    "\n",
    "# for smaller wakari instance\n",
    "#train = train.head(5000)\n",
    "#test = test.head(5000)\n",
    "print train.shape\n",
    "\n",
    "# for real testing set we don't need to do this - it doesn't have place_id info\n",
    "#test = test[['row_id', 'x', 'y', 'accuracy', 'time']]\n",
    "\n",
    "print train.head()\n",
    "print test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import K_fbV.fb_split_grid as sg # single process split grid - with normal dictionary as output\n",
    "import K_fbV.para_split_grid as psg # multi-process split grid - with multiprocess.Manager.dict() object as output\n",
    "import K_fbV.factorize_predictor as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10794, 9)\n",
      "<class 'multiprocessing.managers.DictProxy'>\n"
     ]
    }
   ],
   "source": [
    "# generate the grids\n",
    "train_grid, test_grid = psg.get_grids(train, test, outputFile = True,\n",
    "                                    train_output = config.get(\"path\", \"train_grid_path\"),\n",
    "                                    test_output = config.get(\"path\", \"test_grid_path\"), n = 50, m = 50)\n",
    "\n",
    "print train_grid[(2,3)].shape # number of testing set data points in grid (2, 3)\n",
    "print type(train_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import K_fbV.feature_predictor_split as fps\n",
    "import K_fbV.tune_numRound_maxDepth as tnm\n",
    "\n",
    "import K_fbV.train_grid_model as tgm\n",
    "import K_fbV.make_grid_prediction as mgp\n",
    "\n",
    "import K_fbV.gen_submission as gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take only top 80 percentile in all training set grids for modeling - WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8654, 9)\n"
     ]
    }
   ],
   "source": [
    "import K_fbV.top_k_percentil_in_grid as tkpig\n",
    "\n",
    "for tup in train_grid.keys():\n",
    "    train_grid[tup] = tkpig.top_K_percentile_pid(train_grid[tup], k_p = 80, pid = 'place_id')\n",
    "    \n",
    "print train_grid[(2,3)].shape # number of training set data points in grid (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#import itertools\n",
    "\n",
    "#p = Pool()                                   #number of processes = number of CPUs\n",
    "#keys, values= zip(*train_grid.iteritems())            #ordered keys and values\n",
    "#processed_values = p.map( tkpig.top_K_percentile_pid, \n",
    "#                         itertools.izip\n",
    "#                        (\n",
    "#                            train_grid.keys(),\n",
    "#                            itertools.repeat(80),\n",
    "#                            itertools.repeat('place_id') \n",
    "#                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param['eta'] = float(param['eta']) # Parameters for Tree Booster - Booster parameter\n",
    "param['silent'] = int(param['silent']) # whether to print logs\n",
    "param['nthread'] = int(param['nthread']) # parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Quick smoke test of the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.940825\n",
      "[1]\ttrain-mlogloss:2.636693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of boosters in training: 3\n",
      "number of boosters in training: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]\ttrain-mlogloss:2.422359\n",
      "[0]\ttrain-mlogloss:3.212625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manager = Manager()\n",
    "\n",
    "model_dict = manager.dict() # dictionary object can be shared by multiple processes\n",
    "unique_class_dict = manager.dict() # dictionary object can be shared by multiple processes\n",
    "\n",
    "features = ['x', 'y', 'accuracy', 'time']\n",
    "tgm.train_model((2, 3), train_grid, param, model_dict, unique_class_dict,\n",
    "            feature_list = features, predictor_name = 'place_id',\n",
    "            dump_model_txt = True, model_txt_file = config.get(\"path\", \"model_txt_path\"),\n",
    "            save_model = True, save_model_file = config.get(\"path\", \"model_bite_path\"),\n",
    "            leaf_mltpr = 3.01, tree_mltpr = 8.01)\n",
    "\n",
    "features = ['x', 'y', 'accuracy', 'time']\n",
    "tgm.train_model((2, 2), train_grid, param, model_dict, unique_class_dict,\n",
    "            feature_list = features, predictor_name = 'place_id',\n",
    "            dump_model_txt = True, model_txt_file = config.get(\"path\", \"model_txt_path\"),\n",
    "            save_model = True, save_model_file = config.get(\"path\", \"model_bite_path\"),\n",
    "            leaf_mltpr = 1.01, tree_mltpr = 1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_dict = manager.dict() # dictionary object can be shared by multiple processes\n",
    "\n",
    "#features = ['x', 'y', 'accuracy', 'time']\n",
    "features = ['x', 'y', 'accuracy', 'time', 'hour', 'weekday', 'month']\n",
    "mgp.predict_output((2, 3), test_grid, model_dict, unique_class_dict, prediction_dict,\n",
    "               test_row_id = 'row_id', feature_list = features,\n",
    "               saveFile = True, outputPath = config.get(\"path\", \"grid_output_path\"))\n",
    "\n",
    "#features = ['x', 'y', 'accuracy', 'time']\n",
    "features = ['x', 'y', 'accuracy', 'time', 'hour', 'weekday', 'month']\n",
    "mgp.predict_output((2, 2), test_grid, model_dict, unique_class_dict, prediction_dict,\n",
    "               test_row_id = 'row_id', feature_list = features,\n",
    "               saveFile = True, outputPath = config.get(\"path\", \"grid_output_path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id             0             1             2\n",
      "2836      95  9.682966e+09  3.513194e+09  7.809444e+09\n",
      "0       1698  2.405501e+09  8.617309e+09  7.964007e+09\n",
      "1       1726  4.608206e+09  8.050670e+09  1.657333e+09\n",
      "2837    2354  3.327668e+09  7.268641e+09  1.091456e+09\n",
      "2       3310  4.682130e+09  3.676443e+09  8.947502e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K_fbV/gen_submission.py:8: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  df_finalResult = df_finalResult.sort(['row_id'])\n"
     ]
    }
   ],
   "source": [
    "finalResult = gs.merge_prediction_grids(prediction_dict, save_file = True, output_path = config.get(\"path\", \"output_path\"))\n",
    "print finalResult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-process Parallalization of the above pipeline to use all cores\n",
    "- specify a pool of processes using multiprocess package\n",
    "- need wrapper a function for each function with input parameters more than 1\n",
    "- use itertools to zip arguments in a list, and in wrapper function unzip/unwrap it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def fn_timer(function):\n",
    "    @wraps(function)\n",
    "    def function_timer(*args, **kwargs):\n",
    "        t0 = time.time()\n",
    "        result = function(*args, **kwargs)\n",
    "        t1 = time.time()\n",
    "        print (\"Total time running %s: %s seconds\" %\n",
    "               (function.func_name, str(t1-t0))\n",
    "               )\n",
    "        return result\n",
    "    return function_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parallel_train_wrap(inputs): # original function wrapper for catching exceptions and unpack input list\n",
    "    try:\n",
    "        \"\"\"Convert f([1,2]) to f(1,2) call\"\"\"\n",
    "        print \"traninig a grid...\"\n",
    "        tgm.train_model(*inputs) # original function\n",
    "    except:\n",
    "        print inputs[0] # print the tuple value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import *\n",
    "from multiprocessing import Queue, Process, freeze_support\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import itertools # for multiple input arguments of the function to be parallilized\n",
    "\n",
    "print multiprocessing.cpu_count() # print the number of CPUs availalbe to system\n",
    "\n",
    "# define input arguments with their associated names in function\n",
    "feature_list = ['x', 'y', 'accuracy', 'time']\n",
    "predictor_name = 'place_id'\n",
    "leaf_mltpr = 1.002\n",
    "tree_mltpr = 1.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@fn_timer\n",
    "def timed_parallel_train_wrap():\n",
    "    pool = Pool( processes = int(multiprocessing.cpu_count()) ) # process pool based on number of cores\n",
    "    pool.map\n",
    "    (\n",
    "        parallel_train_wrap, # function to be parallilized\n",
    "        itertools.izip\n",
    "        (\n",
    "            train_grid.keys(),\n",
    "            itertools.repeat(train_grid),\n",
    "            itertools.repeat(param),\n",
    "            itertools.repeat(model_dict),\n",
    "            itertools.repeat(unique_class_dict),\n",
    "            itertools.repeat(feature_list),\n",
    "            itertools.repeat(predictor_name),\n",
    "            itertools.repeat(leaf_mltpr),\n",
    "            itertools.repeat(tree_mltpr)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timed_parallel_train_wrap()\n",
    "print model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
